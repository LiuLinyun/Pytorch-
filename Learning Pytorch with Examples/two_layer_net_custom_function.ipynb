{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "PyTorch: Defining new autograd functions\n",
    "----------------------------------------\n",
    "\n",
    "A fully-connected ReLU network with one hidden layer and no biases, trained to\n",
    "predict y from x by minimizing squared Euclidean distance.\n",
    "\n",
    "This implementation computes the forward pass using operations on PyTorch\n",
    "Variables, and uses PyTorch autograd to compute gradients.\n",
    "\n",
    "In this implementation we implement our own custom autograd function to perform\n",
    "the ReLU function.  \n",
    "在这个实现里我们使用自定义的自动求梯度函数来实现ReLU函数。\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 28735342.0\n",
      "1 25246106.0\n",
      "2 27532936.0\n",
      "3 31889838.0\n",
      "4 33569268.0\n",
      "5 29356610.0\n",
      "6 19958198.0\n",
      "7 10998290.0\n",
      "8 5355120.0\n",
      "9 2658540.5\n",
      "10 1478403.375\n",
      "11 955759.625\n",
      "12 698090.1875\n",
      "13 551156.625\n",
      "14 454357.59375\n",
      "15 383542.5625\n",
      "16 328383.375\n",
      "17 283668.375\n",
      "18 246613.234375\n",
      "19 215509.34375\n",
      "20 189178.484375\n",
      "21 166770.90625\n",
      "22 147557.671875\n",
      "23 130992.8828125\n",
      "24 116646.1484375\n",
      "25 104190.0625\n",
      "26 93331.625\n",
      "27 83818.6875\n",
      "28 75473.5\n",
      "29 68111.4296875\n",
      "30 61598.8984375\n",
      "31 55818.98046875\n",
      "32 50676.921875\n",
      "33 46098.1875\n",
      "34 42014.83984375\n",
      "35 38355.72265625\n",
      "36 35068.59765625\n",
      "37 32109.373046875\n",
      "38 29443.935546875\n",
      "39 27039.138671875\n",
      "40 24862.625\n",
      "41 22893.318359375\n",
      "42 21106.31640625\n",
      "43 19481.04296875\n",
      "44 18000.00390625\n",
      "45 16649.380859375\n",
      "46 15415.1494140625\n",
      "47 14285.85546875\n",
      "48 13251.541015625\n",
      "49 12302.4248046875\n",
      "50 11430.7529296875\n",
      "51 10629.4423828125\n",
      "52 9892.138671875\n",
      "53 9215.3759765625\n",
      "54 8590.830078125\n",
      "55 8014.16845703125\n",
      "56 7481.2080078125\n",
      "57 6988.34130859375\n",
      "58 6532.08447265625\n",
      "59 6109.15625\n",
      "60 5717.05517578125\n",
      "61 5352.9248046875\n",
      "62 5014.6904296875\n",
      "63 4700.3095703125\n",
      "64 4407.13916015625\n",
      "65 4134.31103515625\n",
      "66 3880.211669921875\n",
      "67 3643.1640625\n",
      "68 3422.100830078125\n",
      "69 3215.8173828125\n",
      "70 3023.22998046875\n",
      "71 2843.287841796875\n",
      "72 2675.025634765625\n",
      "73 2517.70166015625\n",
      "74 2370.454345703125\n",
      "75 2232.589599609375\n",
      "76 2103.4443359375\n",
      "77 1982.389892578125\n",
      "78 1868.9822998046875\n",
      "79 1762.4290771484375\n",
      "80 1662.4627685546875\n",
      "81 1568.6361083984375\n",
      "82 1480.5174560546875\n",
      "83 1397.7313232421875\n",
      "84 1319.9200439453125\n",
      "85 1246.7855224609375\n",
      "86 1177.993896484375\n",
      "87 1113.2801513671875\n",
      "88 1052.353515625\n",
      "89 994.9917602539062\n",
      "90 940.9658813476562\n",
      "91 890.0748291015625\n",
      "92 842.1160888671875\n",
      "93 796.8796997070312\n",
      "94 754.2177734375\n",
      "95 713.9887084960938\n",
      "96 676.023193359375\n",
      "97 640.1896362304688\n",
      "98 606.3812866210938\n",
      "99 574.4468383789062\n",
      "100 544.286376953125\n",
      "101 515.7924194335938\n",
      "102 488.871826171875\n",
      "103 463.4290771484375\n",
      "104 439.3749694824219\n",
      "105 416.6346435546875\n",
      "106 395.12872314453125\n",
      "107 374.7865905761719\n",
      "108 355.5411376953125\n",
      "109 337.33447265625\n",
      "110 320.1011657714844\n",
      "111 303.7860107421875\n",
      "112 288.3423156738281\n",
      "113 273.72113037109375\n",
      "114 259.8700866699219\n",
      "115 246.74911499023438\n",
      "116 234.3195037841797\n",
      "117 222.5389404296875\n",
      "118 211.37515258789062\n",
      "119 200.80068969726562\n",
      "120 190.77093505859375\n",
      "121 181.26194763183594\n",
      "122 172.2453155517578\n",
      "123 163.69444274902344\n",
      "124 155.58511352539062\n",
      "125 147.89080810546875\n",
      "126 140.5911865234375\n",
      "127 133.6648712158203\n",
      "128 127.0920181274414\n",
      "129 120.8572998046875\n",
      "130 114.93634033203125\n",
      "131 109.32131958007812\n",
      "132 103.99246215820312\n",
      "133 98.93204498291016\n",
      "134 94.12623596191406\n",
      "135 89.56044006347656\n",
      "136 85.22578430175781\n",
      "137 81.10588836669922\n",
      "138 77.1915283203125\n",
      "139 73.47494506835938\n",
      "140 69.9410171508789\n",
      "141 66.58216094970703\n",
      "142 63.3899040222168\n",
      "143 60.35490798950195\n",
      "144 57.470489501953125\n",
      "145 54.72708511352539\n",
      "146 52.11956787109375\n",
      "147 49.638267517089844\n",
      "148 47.27987289428711\n",
      "149 45.03657531738281\n",
      "150 42.90251922607422\n",
      "151 40.871978759765625\n",
      "152 38.94093322753906\n",
      "153 37.103240966796875\n",
      "154 35.354148864746094\n",
      "155 33.691009521484375\n",
      "156 32.106998443603516\n",
      "157 30.600297927856445\n",
      "158 29.165437698364258\n",
      "159 27.800519943237305\n",
      "160 26.500396728515625\n",
      "161 25.261966705322266\n",
      "162 24.08405113220215\n",
      "163 22.961423873901367\n",
      "164 21.89282989501953\n",
      "165 20.875246047973633\n",
      "166 19.90675926208496\n",
      "167 18.983606338500977\n",
      "168 18.10442543029785\n",
      "169 17.266860961914062\n",
      "170 16.468656539916992\n",
      "171 15.7081880569458\n",
      "172 14.984275817871094\n",
      "173 14.29372501373291\n",
      "174 13.636725425720215\n",
      "175 13.01024055480957\n",
      "176 12.41306209564209\n",
      "177 11.843498229980469\n",
      "178 11.30090618133545\n",
      "179 10.783822059631348\n",
      "180 10.290763854980469\n",
      "181 9.820642471313477\n",
      "182 9.372733116149902\n",
      "183 8.945868492126465\n",
      "184 8.538653373718262\n",
      "185 8.150054931640625\n",
      "186 7.779674530029297\n",
      "187 7.426568508148193\n",
      "188 7.089587211608887\n",
      "189 6.768399238586426\n",
      "190 6.462066173553467\n",
      "191 6.169956684112549\n",
      "192 5.891225337982178\n",
      "193 5.625277996063232\n",
      "194 5.371521949768066\n",
      "195 5.129538059234619\n",
      "196 4.898654937744141\n",
      "197 4.678349018096924\n",
      "198 4.468092441558838\n",
      "199 4.267473220825195\n",
      "200 4.075972080230713\n",
      "201 3.893366575241089\n",
      "202 3.7190749645233154\n",
      "203 3.5525095462799072\n",
      "204 3.3937861919403076\n",
      "205 3.242295265197754\n",
      "206 3.0974810123443604\n",
      "207 2.959432601928711\n",
      "208 2.827496290206909\n",
      "209 2.701659917831421\n",
      "210 2.581486701965332\n",
      "211 2.4666366577148438\n",
      "212 2.3572030067443848\n",
      "213 2.252565622329712\n",
      "214 2.1528425216674805\n",
      "215 2.0573718547821045\n",
      "216 1.966202974319458\n",
      "217 1.879104495048523\n",
      "218 1.7961187362670898\n",
      "219 1.716821551322937\n",
      "220 1.6410311460494995\n",
      "221 1.5686644315719604\n",
      "222 1.4994251728057861\n",
      "223 1.4334075450897217\n",
      "224 1.3703818321228027\n",
      "225 1.3100411891937256\n",
      "226 1.2524789571762085\n",
      "227 1.1974761486053467\n",
      "228 1.1449605226516724\n",
      "229 1.0947524309158325\n",
      "230 1.0467817783355713\n",
      "231 1.0009180307388306\n",
      "232 0.9570034742355347\n",
      "233 0.9152674674987793\n",
      "234 0.8751731514930725\n",
      "235 0.8370135426521301\n",
      "236 0.8005601167678833\n",
      "237 0.765576958656311\n",
      "238 0.7321950197219849\n",
      "239 0.7002710700035095\n",
      "240 0.669776439666748\n",
      "241 0.6406625509262085\n",
      "242 0.6128413081169128\n",
      "243 0.5861743688583374\n",
      "244 0.5606738924980164\n",
      "245 0.5363535284996033\n",
      "246 0.5130916237831116\n",
      "247 0.49091261625289917\n",
      "248 0.46962329745292664\n",
      "249 0.4493035674095154\n",
      "250 0.4298289120197296\n",
      "251 0.4112865924835205\n",
      "252 0.3935193419456482\n",
      "253 0.37650012969970703\n",
      "254 0.36020970344543457\n",
      "255 0.3446693420410156\n",
      "256 0.32984501123428345\n",
      "257 0.31566494703292847\n",
      "258 0.3019976019859314\n",
      "259 0.2890222668647766\n",
      "260 0.27658966183662415\n",
      "261 0.26468902826309204\n",
      "262 0.25335511565208435\n",
      "263 0.24245917797088623\n",
      "264 0.23204806447029114\n",
      "265 0.22211159765720367\n",
      "266 0.21257592737674713\n",
      "267 0.2034485936164856\n",
      "268 0.19476155936717987\n",
      "269 0.1864241510629654\n",
      "270 0.17844875156879425\n",
      "271 0.1708153635263443\n",
      "272 0.16353744268417358\n",
      "273 0.15655457973480225\n",
      "274 0.1498761773109436\n",
      "275 0.14347489178180695\n",
      "276 0.13734835386276245\n",
      "277 0.13151520490646362\n",
      "278 0.1259104311466217\n",
      "279 0.12053188681602478\n",
      "280 0.11540970951318741\n",
      "281 0.110499307513237\n",
      "282 0.10580463707447052\n",
      "283 0.10130321234464645\n",
      "284 0.0969996228814125\n",
      "285 0.09287714213132858\n",
      "286 0.08894117176532745\n",
      "287 0.08517760783433914\n",
      "288 0.08156460523605347\n",
      "289 0.07812856882810593\n",
      "290 0.07480968534946442\n",
      "291 0.0716530829668045\n",
      "292 0.06862008571624756\n",
      "293 0.06572294980287552\n",
      "294 0.06294834613800049\n",
      "295 0.060293909162282944\n",
      "296 0.05776194483041763\n",
      "297 0.05532865598797798\n",
      "298 0.052996471524238586\n",
      "299 0.050760459154844284\n",
      "300 0.048635732382535934\n",
      "301 0.046590935438871384\n",
      "302 0.04463023692369461\n",
      "303 0.04276183247566223\n",
      "304 0.04096262902021408\n",
      "305 0.0392477922141552\n",
      "306 0.03760581836104393\n",
      "307 0.03602353483438492\n",
      "308 0.03450483828783035\n",
      "309 0.03307214379310608\n",
      "310 0.03169327601790428\n",
      "311 0.030362574383616447\n",
      "312 0.029094303026795387\n",
      "313 0.027878960594534874\n",
      "314 0.026723552495241165\n",
      "315 0.0256094578653574\n",
      "316 0.024547409266233444\n",
      "317 0.023528864607214928\n",
      "318 0.022550970315933228\n",
      "319 0.021618641912937164\n",
      "320 0.020721277222037315\n",
      "321 0.019864924252033234\n",
      "322 0.019039472565054893\n",
      "323 0.01825534552335739\n",
      "324 0.017505288124084473\n",
      "325 0.01678593084216118\n",
      "326 0.016094021499156952\n",
      "327 0.015437870286405087\n",
      "328 0.014807559549808502\n",
      "329 0.014199333265423775\n",
      "330 0.013621279038488865\n",
      "331 0.013065253384411335\n",
      "332 0.01253827940672636\n",
      "333 0.01202690601348877\n",
      "334 0.011534914374351501\n",
      "335 0.011069097556173801\n",
      "336 0.010618120431900024\n",
      "337 0.010192460380494595\n",
      "338 0.009781989268958569\n",
      "339 0.00938532967120409\n",
      "340 0.009006652049720287\n",
      "341 0.008646192960441113\n",
      "342 0.008297639898955822\n",
      "343 0.007967408746480942\n",
      "344 0.007650311104953289\n",
      "345 0.007347316946834326\n",
      "346 0.007054547313600779\n",
      "347 0.006780136376619339\n",
      "348 0.006511180195957422\n",
      "349 0.006256601307541132\n",
      "350 0.006011775229126215\n",
      "351 0.0057771457359194756\n",
      "352 0.005552119575440884\n",
      "353 0.005335437133908272\n",
      "354 0.005123440641909838\n",
      "355 0.004925599321722984\n",
      "356 0.004738474264740944\n",
      "357 0.004553222097456455\n",
      "358 0.004377102479338646\n",
      "359 0.0042122239246964455\n",
      "360 0.004052347503602505\n",
      "361 0.0038987668231129646\n",
      "362 0.003752977354452014\n",
      "363 0.003611758118495345\n",
      "364 0.0034763349685817957\n",
      "365 0.0033466676250100136\n",
      "366 0.003220940940082073\n",
      "367 0.00310122431255877\n",
      "368 0.0029828790575265884\n",
      "369 0.002873391378670931\n",
      "370 0.002767920261248946\n",
      "371 0.0026681669987738132\n",
      "372 0.002568944124504924\n",
      "373 0.0024752456229180098\n",
      "374 0.0023881683591753244\n",
      "375 0.0023057544603943825\n",
      "376 0.002219254383817315\n",
      "377 0.0021420633420348167\n",
      "378 0.0020633984822779894\n",
      "379 0.0019937537144869566\n",
      "380 0.0019239397952333093\n",
      "381 0.0018575970316305757\n",
      "382 0.0017923350678756833\n",
      "383 0.0017298836028203368\n",
      "384 0.001670147990807891\n",
      "385 0.001614258741028607\n",
      "386 0.0015607186360284686\n",
      "387 0.0015074298717081547\n",
      "388 0.0014576989924535155\n",
      "389 0.0014078067615628242\n",
      "390 0.0013620690442621708\n",
      "391 0.0013157146750018\n",
      "392 0.0012744269333779812\n",
      "393 0.001232784241437912\n",
      "394 0.0011922401608899236\n",
      "395 0.0011546188034117222\n",
      "396 0.0011169721838086843\n",
      "397 0.0010824956698343158\n",
      "398 0.0010479578049853444\n",
      "399 0.0010158387012779713\n",
      "400 0.0009826062014326453\n",
      "401 0.0009514290140941739\n",
      "402 0.0009222989901900291\n",
      "403 0.0008933727513067424\n",
      "404 0.0008665707428008318\n",
      "405 0.0008405937696807086\n",
      "406 0.0008142564329318702\n",
      "407 0.0007909549167379737\n",
      "408 0.0007665939047001302\n",
      "409 0.0007447354728356004\n",
      "410 0.0007228238391689956\n",
      "411 0.0007010044646449387\n",
      "412 0.0006806158926337957\n",
      "413 0.0006621253560297191\n",
      "414 0.0006432161317206919\n",
      "415 0.0006245670956559479\n",
      "416 0.0006071678944863379\n",
      "417 0.0005904412246309221\n",
      "418 0.0005741794011555612\n",
      "419 0.0005594167741946876\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "420 0.0005435514030978084\n",
      "421 0.0005301890196278691\n",
      "422 0.0005146274925209582\n",
      "423 0.0005001294193789363\n",
      "424 0.000486225908389315\n",
      "425 0.00047359461314044893\n",
      "426 0.0004609685856848955\n",
      "427 0.00044915496255271137\n",
      "428 0.0004376848228275776\n",
      "429 0.00042648735688999295\n",
      "430 0.00041561017860658467\n",
      "431 0.0004047971742693335\n",
      "432 0.0003946475626435131\n",
      "433 0.0003849180357065052\n",
      "434 0.00037521147169172764\n",
      "435 0.0003658151545096189\n",
      "436 0.00035692405072040856\n",
      "437 0.00034788562334142625\n",
      "438 0.00033956323750317097\n",
      "439 0.0003313202760182321\n",
      "440 0.0003237763012293726\n",
      "441 0.0003159889602102339\n",
      "442 0.0003081181494053453\n",
      "443 0.0003007858758792281\n",
      "444 0.0002939070691354573\n",
      "445 0.00028729953919537365\n",
      "446 0.0002804831601679325\n",
      "447 0.00027427496388554573\n",
      "448 0.0002672542759682983\n",
      "449 0.0002615601406432688\n",
      "450 0.0002556566323619336\n",
      "451 0.00025069070397876203\n",
      "452 0.00024463451700285077\n",
      "453 0.00023971364134922624\n",
      "454 0.00023463134129997343\n",
      "455 0.00022954642190597951\n",
      "456 0.00022464692301582545\n",
      "457 0.0002200302988057956\n",
      "458 0.00021557412401307374\n",
      "459 0.00021095963893458247\n",
      "460 0.00020709012460429221\n",
      "461 0.00020261816098354757\n",
      "462 0.00019849675300065428\n",
      "463 0.00019482054631225765\n",
      "464 0.00019059052283409983\n",
      "465 0.00018690591969061643\n",
      "466 0.00018333509797230363\n",
      "467 0.00017922450206242502\n",
      "468 0.00017616836703382432\n",
      "469 0.00017289018433075398\n",
      "470 0.00016904647054616362\n",
      "471 0.0001658896217122674\n",
      "472 0.00016271191998384893\n",
      "473 0.00015955528942868114\n",
      "474 0.00015639177581761032\n",
      "475 0.00015345014980994165\n",
      "476 0.00015051104128360748\n",
      "477 0.0001477251062169671\n",
      "478 0.000144946898217313\n",
      "479 0.00014236847346182913\n",
      "480 0.00013979330833535641\n",
      "481 0.00013732281513512135\n",
      "482 0.00013487784599419683\n",
      "483 0.00013241902342997491\n",
      "484 0.00012991999392397702\n",
      "485 0.00012758573575410992\n",
      "486 0.00012530802632682025\n",
      "487 0.00012301992683205754\n",
      "488 0.00012108304508728907\n",
      "489 0.00011895903298864141\n",
      "490 0.00011705498036462814\n",
      "491 0.00011536719830473885\n",
      "492 0.00011354774323990569\n",
      "493 0.00011174572864547372\n",
      "494 0.00010957188351312652\n",
      "495 0.00010788904182845727\n",
      "496 0.00010619329987093806\n",
      "497 0.000104736172943376\n",
      "498 0.00010278170520905405\n",
      "499 0.00010104493412654847\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch.autograd import Variable\n",
    "\n",
    "\n",
    "class MyReLU(torch.autograd.Function):  #继承torch.autograd.Function\n",
    "    \"\"\"\n",
    "    We can implement our own custom autograd Functions by subclassing\n",
    "    torch.autograd.Function and implementing the forward and backward passes\n",
    "    which operate on Tensors.  \n",
    "    我们可以实现自定义的自动求导函数，通过继承torch.autograd.Function，然后重写在 Tensor 上运行的 forward 和 backward 方法。\n",
    "    \"\"\"\n",
    "\n",
    "    def forward(self, input):  #实现自己的forward操作\n",
    "        \"\"\"\n",
    "        In the forward pass we receive a Tensor containing the input and return a\n",
    "        Tensor containing the output. You can cache arbitrary Tensors for use in the\n",
    "        backward pass using the save_for_backward method.  \n",
    "        在前向计算中我们接受一个 Tensor 作为输入参数，并返回一个 Tensor 作为输出参数。\n",
    "        你可以使用 save_for_backward 方法缓存任意的 Tensor 在反向计算中使用。\n",
    "        \"\"\"\n",
    "        self.save_for_backward(input)  #saved for use in the first line of the backward pass\n",
    "        return input.clamp(min=0)\n",
    "\n",
    "    def backward(self, grad_output):    #实现自己的backward操作\n",
    "        \"\"\"\n",
    "        In the backward pass we receive a Tensor containing the gradient of the loss\n",
    "        with respect to the output, and we need to compute the gradient of the loss\n",
    "        with respect to the input.  \n",
    "        在反向计算中，我们接受一个包含dL/doutput的梯度的Tensor，我们需要计算dL/dinput的梯度，即为(dL/doutput)*(doutput/dinput)。\n",
    "        grad_output 应该是计算图后面反向传过来的，就像这个函数反向传到前面一样。\n",
    "        \"\"\"\n",
    "        input, = self.saved_tensors\n",
    "        grad_input = grad_output.clone()\n",
    "        grad_input[input < 0] = 0\n",
    "        return grad_input\n",
    "\n",
    "\n",
    "dtype = torch.FloatTensor\n",
    "dtype = torch.cuda.FloatTensor # Uncomment this to run on GPU\n",
    "\n",
    "# N is batch size; D_in is input dimension;\n",
    "# H is hidden dimension; D_out is output dimension.\n",
    "N, D_in, H, D_out = 64, 1000, 100, 10\n",
    "\n",
    "# Create random Tensors to hold input and outputs, and wrap them in Variables.\n",
    "x = Variable(torch.randn(N, D_in).type(dtype), requires_grad=False)\n",
    "y = Variable(torch.randn(N, D_out).type(dtype), requires_grad=False)\n",
    "\n",
    "# Create random Tensors for weights, and wrap them in Variables.\n",
    "w1 = Variable(torch.randn(D_in, H).type(dtype), requires_grad=True)\n",
    "w2 = Variable(torch.randn(H, D_out).type(dtype), requires_grad=True)\n",
    "\n",
    "learning_rate = 1e-6\n",
    "for t in range(500):\n",
    "    # Construct an instance of our MyReLU class to use in our network\n",
    "    relu = MyReLU()\n",
    "\n",
    "    # Forward pass: compute predicted y using operations on Variables; we compute\n",
    "    # ReLU using our custom autograd operation.\n",
    "    y_pred = relu(x.mm(w1)).mm(w2)\n",
    "\n",
    "    # Compute and print loss\n",
    "    loss = (y_pred - y).pow(2).sum()\n",
    "    print(t, loss.data[0])\n",
    "\n",
    "    # Use autograd to compute the backward pass.\n",
    "    loss.backward()\n",
    "\n",
    "    # Update weights using gradient descent\n",
    "    w1.data -= learning_rate * w1.grad.data\n",
    "    w2.data -= learning_rate * w2.grad.data\n",
    "\n",
    "    # Manually zero the gradients after updating weights\n",
    "    w1.grad.data.zero_()\n",
    "    w2.grad.data.zero_()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
